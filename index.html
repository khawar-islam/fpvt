<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Face Pyramid Vision Transformer</title>
  <!--=================Meta tags==========================-->
  <meta name="robots" content="index,follow">
  <meta name="description"
    content="Face Pyramid Vision Transformer">
  <meta name="keywords" content="face recognition, age-invariant face recongition, vision transformer, pyramid vision transformerFPVT">
  <link rel="author" href="https://khawar-islam.github.io/">
  <!--=================js==========================-->
  <link href="./css.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" type="text/css" href="main.css" media="screen">
  <script src="./effect.js "></script>
  <!-- Latex -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
        TeX: { equationNumbers: { autoNumber: "AMS" } },
      });
      </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
    </script>
  <!--=================Google Analytics==========================-->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-129775907-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'UA-129775907-1');
  </script>
</head>

<body>
  <div id="content">
    <div id="content-inner">
      <div class="section head">
        <h1>
          <font color="Tomato">FPVT</font>--Face Pyramid Vision Transformer
        </h1>

        <div class="authors">
          <a href="https://khawar-islam.github.io/" target="_blank">Khawar Islam</a> <sup>1</sup>
          &nbsp;&nbsp;&nbsp;&nbsp;
          <a href="https://zaighamz.com/" target="_blank">Muhammad Zaigham Zaheer</a> <sup>2,4</sup> &nbsp;&nbsp;&nbsp;&nbsp;
          <a href="https://itu.edu.pk/faculty-itu/dr-arif-mahmood/" target="_blank">Arif Mahmood</a> <sup>3</sup>
        </div>

        <div class="affiliations ">
          <sup>1</sup> FloppyDisk.AI, Pakistan &nbsp;
          <sup>2</sup> Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE <br>
          <sup>3</sup> Information Technology University, Pakistan &nbsp;
          <sup>4</sup> Electronics and Telecommunications Research Institute, South Korea <br>
        </div>

        <!--=================Conference Name==========================-->
        <div class="authors">
          <a href="https://bmvc2022.org/programme/papers/" target="_blank">British Machine Vision Conference (BMVC) 2022</a>
        </div>
        <!--=================Conference Name==========================-->


        <div class="link">
          <a href="https://openaccess.thecvf.com/content/CVPR2021W/CLIC/papers/Islam_Image_Compression_With_Recurrent_Neural_Network_and_Generalized_Divisive_Normalization_CVPRW_2021_paper.pdf" target="_blank">[Paper]</a>&nbsp;
          <a href="https://github.com/khawar-islam/FPVT_BMVC22" target="_blank">[Code]</a>
          <a href="https://youtu.be/B02dOB0PXxk" target="_blank">[Video]</a>
          <a href="./bibtex.txt" target="_blank">[Bibtex]</a>
      
        </div>
       
<!--=================Tabs==========================-->
        <ul id="tabs">
          <li><a href="#Highlights" name="#tab1">Highlights</a></li>
          <li><a href="#Evaluation & Results" name="#tab2">Evaluation & Results</a></li>
          <li><a href="#Citation" name="#tab3">Citation</a></li>
          <li><a href="#Contact" name="#tab4">Contact</a></li>
      </div>
      <br>
      <!--=================Teasers==========================-->
      <div class="section abstract" class="img_container">
        <center><img src="./fpvt.png" border="0" width="95%"></center>
        <br>
        <p style="text-align:justify">
          A simplified view of Face Pyramid Vision Transformer (FPVT) with limited computational resources. 
          The whole network represents a pyramid structure, every stage comprises of an improved patch embedding layer 
          and encoder layer.Following progressive shrinking strategy, the output resolution is diverse at every stage from high to 
          low resolution.
        </p>
      </div>
      
       <!--=================News==========================-->
      <div class="section Highlights", id="Highlights">
        <h2><font color="red">News</font></h2>
        <ol>
          <li>6/10/2022: We will release the extended version of our FPVT paper soon.</li>
        </ol>
      </div>
        
      <!--=================Highlights==========================-->
      <div class="section Highlights", id="Highlights">
        <h2>Highlights</h2>
        <ol>
          <li>FPVT introduces a CFFN that extracts locality information</li>
          <li>We present Face Pyramid Vision Transformer (FPVT) to learn multi-scale discriminative</li>
          <li>We introduce IPE which utilizes all benefits of CNNs to model low-level edges to higher-level semantic primitives</li>
        </ol>
      </div>
      
      <!--=================Abstract==========================-->
      <div class="section abstract">
        <h2>Abstract</h2>
        <br>
        <p style="text-align:justify">
          A novel Face Pyramid Vision Transformer (FPVT) is proposed to learn a discriminative multi-scale facial representations for face recognition and verification. In FPVT,
          Face Spatial Reduction Attention (FSRA) and Dimensionality Reduction (FDR) layers
          are employed to reduce the computations by compacting the feature maps. An Improved Patch Embedding (IPE) algorithm is proposed to exploit the benefits of CNNs in
          ViTs (e.g., shared weights, local context, and receptive fields) to model low-level edges to higher-level semantic primitives. Within FPVT framework, a Convolutional Feed-
          Forward Network (CFFN) is proposed that extracts locality information to learn low
          level facial information. The proposed FPVT is evaluated on seven benchmark datasets
          and compared with ten existing state-of-the-art methods including CNNs, pure ViTs and
          Convolutional ViTs. Despite fewer parameters, FPVT has demonstrated excellent performance over the compared methods.
        </p>
      </div>
        
     <!--=================Experiments==========================-->
      <div class="section Evaluation & Results" , id="Evaluation & Results">
        <h2>Evaluation & Results</h2>
        <p style="text-align:justify">
          We conduct multiple ablation studies to validate the impact of our proposed work in our FPVT modules. 
          Table. 1 and Table 2 present the results of CNNs, pure ViTs, and Convolutional ViTs on seven datasets. 
          PVT (referred to as baseline) is a standard pure pyramid transformer without convolution. 
          We choose PVT as a baseline due to two main reasons i) it generates multi-scale features 
          ii) the number of parameters is larger than ResNet18. 
          The introduction of the IPE block leads to a gain of 1% in terms of performance, highlighting the 
          impact of convolutional tokens. IPE block improves the performance on LFW from 78.8% to 82.9%, 
          CFP_FF from 75.2% to 85.5%, CFP_FP from 52.9% to 65.6%, AgeDB from 59.9% to 65.6%, CALFW from 
          66.8% to 70.1%, and CPLFW from 55.1% to 59%.
        </p>
        <div id="img_intro_examples" class="img_container">
          <center><img src="./figs/table2.png" border="0" width="70%"></center>
          <br>
        <p style="text-align:justify">
          Table 1: Evaluation results on CFP_FF, CFP_FP and VGG2_FP. The top first three models
          lists the results of the CNN models, while the mid-block presents the performance of pure
          ViTs architectures. The third block shows the results of Conv. ViTs models.
        </p>
        </div>
        <br />
        <br />
        <div id="img_intro_examples" class="img_container">
          <center><img src="./figs/table1.png" border="0" width="55%"></center>
          <br>
        <p style="text-align:justify">
          Table 2: Evaluation results on LFW, CALFW, CPLFW and AgeDB: Comparison with FPVT, CNNs, PureViTs and 
          Convolutional ViTs methods. The top first three models represents the results of most representative 
          CNN models, while the mid block indicates the performance of pure ViTs architectures. The third
          block shows the results of Conv. ViTs models
        </p>
        </div>
     </div>
        
      <!--=================Citation==========================-->
      <div class="section Citation" , id="Citation">
        <h2>Citation</h2>
        <div class="section bibtex">
<pre>@InProceedings{Khawar_BMVC22_FPVT,
  author = {Khawar Islam, Muhammad Zaigham Zaheer, Arif Mahmood},
  title = {Face Pyramid Vision Transformer},
  booktitle = {Proceedings of the British Machine Vision Conference},
  year = {2022}
}</pre>
<pre>@inproceedings{islam2021face,
  title={Face Recognition Using Shallow Age-Invariant Data},
  author={Islam, Khawar and Lee, Sujin and Han, Dongil and Moon, Hyeonjoon},
  booktitle={2021 36th International Conference on Image and Vision Computing New Zealand (IVCNZ)},
  pages={1--6},
  year={2021},
  organization={IEEE}
}</pre>
        </div>
     </div>

      <!--=================Contact==========================-->
      <div class="section contact", id="Contact">
        <h2 id="contact">Contact</h2>
        <p style="text-align:justify">If you have any question, please contact to Khawar Islam at
          <strong>khawarr dot islam at gmail dot com</strong>.</p>
      </div>
</body>

</html>
